{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCUDA: Python interface to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCUDA provides an interface from Python to *all* of CUDA. In addition, PyCUDA offers the following niceties:\n",
    "\n",
    "1. **Memory management**: PyCUDA automatically cleans up objects at the end of their lifetimes, allowing you to write leak-free code effortlessly\n",
    "2. **Error checking**: no need to manually check for errors; PyCUDA automatically translates CUDA errors into Python exceptions\n",
    "3. **NumPy interaction:** PyCUDA provides an `ndarray`-like `GPUArray` class, which supports elementwise operations and broadcasting. NumPy arrays can easily be converted to `GPUArray`s and vice-versa\n",
    "4. **Metaprogramming**: PyCUDA allows you to write more flexible, efficient kernels with configurable block sizes, unrolled loops, and configurable data types\n",
    "5. **Rapid prototyping**: Allows rapid development of algorithms/kernels in e.g., Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycuda import autoinit\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda.compiler as compiler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.916251    0.90730104  0.12345119  0.08681051  0.78787607]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(5)\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_gpu *= 2 # executes on GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.83250199  1.81460208  0.24690239  0.17362103  1.57575214]\n"
     ]
    }
   ],
   "source": [
    "print(a_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_text = \"\"\"\n",
    "__global__ void mm(double *A, double *B, double *C)\n",
    "{\n",
    "  \n",
    "  // Block index\n",
    "  const uint bx = blockIdx.x;\n",
    "  const uint by = blockIdx.y;\n",
    "\n",
    "  // Thread index\n",
    "  const uint tx = threadIdx.x;\n",
    "  const uint ty = threadIdx.y;\n",
    "\n",
    "  // Index of the first sub-matrix of A processed by the block\n",
    "  const uint aBegin = %(N)s * %(BLOCK_SIZE)s * by;\n",
    "  // Index of the last sub-matrix of A processed by the block\n",
    "  const uint aEnd = aBegin + %(N)s - 1;\n",
    "  // Step size used to iterate through the sub-matrices of A\n",
    "  const uint aStep = %(BLOCK_SIZE)s;\n",
    "\n",
    "  // Index of the first sub-matrix of B processed by the block\n",
    "  const uint bBegin = %(BLOCK_SIZE)s * bx;\n",
    "  // Step size used to iterate through the sub-matrices of B\n",
    "  const uint bStep = %(BLOCK_SIZE)s * %(N)s;\n",
    "\n",
    "  // The element of the block sub-matrix that is computed\n",
    "  // by the thread\n",
    "  double Csub = 0;\n",
    "  // Loop over all the sub-matrices of A and B required to\n",
    "  // compute the block sub-matrix\n",
    "  for (int a = aBegin, b = bBegin;\n",
    "       a <= aEnd;\n",
    "       a += aStep, b += bStep) \n",
    "    {\n",
    "      // Shared memory for the sub-matrix of A\n",
    "      __shared__ double As[%(BLOCK_SIZE)s][%(BLOCK_SIZE)s];\n",
    "      // Shared memory for the sub-matrix of B\n",
    "      __shared__ double Bs[%(BLOCK_SIZE)s][%(BLOCK_SIZE)s];\n",
    "\n",
    "      // Load the matrices from global memory to shared memory\n",
    "      // each thread loads one element of each matrix\n",
    "      As[ty][tx] = A[a + %(N)s * ty + tx];\n",
    "      Bs[ty][tx] = B[b + %(N)s * ty + tx];\n",
    "      // Synchronize to make sure the matrices are loaded\n",
    "      __syncthreads();\n",
    "\n",
    "      // Multiply the two matrices together;\n",
    "      // each thread computes one element\n",
    "      // of the block sub-matrix\n",
    "      for (int k = 0; k < %(BLOCK_SIZE)s; ++k)\n",
    "        Csub += As[ty][k] * Bs[k][tx];\n",
    "\n",
    "      // Synchronize to make sure that the preceding\n",
    "      // computation is done before loading two new\n",
    "      // sub-matrices of A and B in the next iteration\n",
    "      __syncthreads();\n",
    "    }\n",
    "\n",
    "  // Write the block sub-matrix to global memory;\n",
    "  // each thread writes one element\n",
    "  const uint c = %(N)s * %(BLOCK_SIZE)s * by + %(BLOCK_SIZE)s * bx;\n",
    "  C[c + %(N)s * ty + tx] = Csub;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_text = kernel_text % {\n",
    "    'N': 1024,\n",
    "    'BLOCK_SIZE': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__global__ void mm(double *A, double *B, double *C)\n",
      "{\n",
      "  \n",
      "  // Block index\n",
      "  const uint bx = blockIdx.x;\n",
      "  const uint by = blockIdx.y;\n",
      "\n",
      "  // Thread index\n",
      "  const uint tx = threadIdx.x;\n",
      "  const uint ty = threadIdx.y;\n",
      "\n",
      "  // Index of the first sub-matrix of A processed by the block\n",
      "  const uint aBegin = 1024 * 32 * by;\n",
      "  // Index of the last sub-matrix of A processed by the block\n",
      "  const uint aEnd = aBegin + 1024 - 1;\n",
      "  // Step size used to iterate through the sub-matrices of A\n",
      "  const uint aStep = 32;\n",
      "\n",
      "  // Index of the first sub-matrix of B processed by the block\n",
      "  const uint bBegin = 32 * bx;\n",
      "  // Step size used to iterate through the sub-matrices of B\n",
      "  const uint bStep = 32 * 1024;\n",
      "\n",
      "  // The element of the block sub-matrix that is computed\n",
      "  // by the thread\n",
      "  double Csub = 0;\n",
      "  // Loop over all the sub-matrices of A and B required to\n",
      "  // compute the block sub-matrix\n",
      "  for (int a = aBegin, b = bBegin;\n",
      "       a <= aEnd;\n",
      "       a += aStep, b += bStep) \n",
      "    {\n",
      "      // Shared memory for the sub-matrix of A\n",
      "      __shared__ double As[32][32];\n",
      "      // Shared memory for the sub-matrix of B\n",
      "      __shared__ double Bs[32][32];\n",
      "\n",
      "      // Load the matrices from global memory to shared memory\n",
      "      // each thread loads one element of each matrix\n",
      "      As[ty][tx] = A[a + 1024 * ty + tx];\n",
      "      Bs[ty][tx] = B[b + 1024 * ty + tx];\n",
      "      // Synchronize to make sure the matrices are loaded\n",
      "      __syncthreads();\n",
      "\n",
      "      // Multiply the two matrices together;\n",
      "      // each thread computes one element\n",
      "      // of the block sub-matrix\n",
      "      for (int k = 0; k < 32; ++k)\n",
      "        Csub += As[ty][k] * Bs[k][tx];\n",
      "\n",
      "      // Synchronize to make sure that the preceding\n",
      "      // computation is done before loading two new\n",
      "      // sub-matrices of A and B in the next iteration\n",
      "      __syncthreads();\n",
      "    }\n",
      "\n",
      "  // Write the block sub-matrix to global memory;\n",
      "  // each thread writes one element\n",
      "  const uint c = 1024 * 32 * by + 32 * bx;\n",
      "  C[c + 1024 * ty + tx] = Csub;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(kernel_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = compiler.SourceModule(kernel_text)\n",
    "func = mod.get_function('mm')\n",
    "mm_gpu = func.prepare([np.intp, np.intp, np.intp])\n",
    "# mm_gpu = func.prepare('PPP') # also works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(1024, 1024)\n",
    "b = np.random.rand(1024, 1024)\n",
    "c = np.zeros([1024, 1024], dtype=np.float64)\n",
    "\n",
    "a_d = gpuarray.to_gpu(a)\n",
    "b_d = gpuarray.to_gpu(b)\n",
    "c_d = gpuarray.to_gpu(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mm_gpu.prepared_call((32, 32, 1), (32, 32, 1), a_d.gpudata, b_d.gpudata, c_d.gpudata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 246.85754872  243.95837449  240.59993294 ...,  250.27893264\n",
      "   250.76185246  236.3539642 ]\n",
      " [ 255.2624859   251.66495653  244.46061962 ...,  245.64956071\n",
      "   252.94923314  238.55513108]\n",
      " [ 265.22106953  265.23279476  260.72509666 ...,  264.66311797\n",
      "   271.60582956  259.9790046 ]\n",
      " ..., \n",
      " [ 250.64846012  239.2127767   243.99730386 ...,  240.59742457\n",
      "   251.96533716  241.66013773]\n",
      " [ 264.47017362  255.2815124   252.71112128 ...,  254.16281217  263.9581633\n",
      "   250.13513162]\n",
      " [ 263.97421922  258.632572    251.96585592 ...,  258.2554      262.68873799\n",
      "   249.19610463]]\n"
     ]
    }
   ],
   "source": [
    "print(c_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 246.85754872  243.95837449  240.59993294 ...,  250.27893264\n",
      "   250.76185246  236.3539642 ]\n",
      " [ 255.2624859   251.66495653  244.46061962 ...,  245.64956071\n",
      "   252.94923314  238.55513108]\n",
      " [ 265.22106953  265.23279476  260.72509666 ...,  264.66311797\n",
      "   271.60582956  259.9790046 ]\n",
      " ..., \n",
      " [ 250.64846012  239.2127767   243.99730386 ...,  240.59742457\n",
      "   251.96533716  241.66013773]\n",
      " [ 264.47017362  255.2815124   252.71112128 ...,  254.16281217  263.9581633\n",
      "   250.13513162]\n",
      " [ 263.97421922  258.632572    251.96585592 ...,  258.2554      262.68873799\n",
      "   249.19610463]]\n"
     ]
    }
   ],
   "source": [
    "print(a.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for matrix multiply on GPU: 0.011200428466796874 seconds\n"
     ]
    }
   ],
   "source": [
    "start = cuda.Event()\n",
    "end = cuda.Event()\n",
    "\n",
    "start.record()\n",
    "for i in range(100):\n",
    "    mm_gpu.prepared_call((32, 32, 1), (32, 32, 1), a_d.gpudata, b_d.gpudata, c_d.gpudata)\n",
    "end.record()\n",
    "end.synchronize()\n",
    "\n",
    "print(\"Average time for matrix multiply on GPU: {} seconds\".format(start.time_till(end)*1e-3 / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for matrix multiply on CPU: 0.049225673828125 seconds\n"
     ]
    }
   ],
   "source": [
    "start.record()\n",
    "for i in range(100):\n",
    "    a.dot(b)\n",
    "end.record()\n",
    "end.synchronize()\n",
    "\n",
    "print(\"Average time for matrix multiply on CPU: {} seconds\".format(start.time_till(end)*1e-3 / 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LogicError",
     "evalue": "cuFuncSetBlockShape failed: invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e45c448e4534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmm_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepared_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpudata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpudata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpudata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# wrong block size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/pycuda/lib/python3.6/site-packages/pycuda-2017.1.1-py3.6-linux-x86_64.egg/pycuda/driver.py\u001b[0m in \u001b[0;36mfunction_prepared_call\u001b[0;34m(func, grid, block, *args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_prepared_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_block_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLogicError\u001b[0m: cuFuncSetBlockShape failed: invalid argument"
     ]
    }
   ],
   "source": [
    "mm_gpu.prepared_call((32, 32, 1), (33, 33, 1), a_d.gpudata, b_d.gpudata, c_d.gpudata) # wrong block size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCUDA",
   "language": "python",
   "name": "pycuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
